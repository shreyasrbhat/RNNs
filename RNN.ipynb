{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import csv\n",
    "import nltk\n",
    "import sys\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Extracting sentences from csv file\n",
    "with open(\"reddit.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    read = csv.reader(file, skipinitialspace = True)\n",
    "    next(read)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in read])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "    print(\"number of parsed sentences {}\".format(len(sentences)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Vocabulary info\n",
    "wordtokens = nltk.FreqDist(itertools.chain(*[nltk.word_tokenize(x) for x in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###most common words\n",
    "vocab = wordtokens.most_common(vocabulary_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###replacing words not in vocab as unknown_token\n",
    "tokenised_sent = [nltk.word_tokenize(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenised_sent):\n",
    "    tokenised_sent[i] = [w if w in index_to_word else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenised_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenised_sent])\n",
    "Y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenised_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations:\n",
    "         $s_t = \\tanh(Ux_t + Ws_{t-1})$  \n",
    "         $o_t = softmax(Vs_t)$\n",
    "dimensions:\n",
    "        - x_t: 8000 x 1\n",
    "        - U: 100 x 8000\n",
    "        - W: 100 x 100\n",
    "        - V: 8000 x 100\n",
    "        - o_t: 8000 x 1\n",
    "        - s_t: 100 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_numpy:\n",
    "    def __init__(self, word_dim, hidden_dim=100, truncate=4):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.truncate = truncate\n",
    "        ##initialising parameters\n",
    "#         self.U = np.random.normal(size = (hidden_dim, word_dim))\n",
    "#         self.W = np.random.normal(size = (hidden_dim, hidden_dim))\n",
    "#         self.V = np.random.normal(size = (word_dim, hidden_dim))\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self,array):\n",
    "            return np.exp(array) / np.sum(np.exp(array), axis=0)\n",
    "RNN_numpy.softmax = softmax\n",
    "        \n",
    "        ###method for forward propagation\n",
    "def forward_propagation(self, x):\n",
    "    ##number of time steps\n",
    "    T = len(x)\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    for t in range(T):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + np.dot(self.W, s[t-1])) ##(100 x 8000) . (8000 x 1) + (100 x 100) . (100 x 1) = (100 x 1)\n",
    "        o[t] = self.softmax(np.dot(self.V, s[t]))                    ##(8000 x 100) + (100 x 1) =(8000 x 1)\n",
    "\n",
    "    return [o,s]\n",
    "RNN_numpy.forward_propagation = forward_propagation\n",
    "\n",
    "#prediction\n",
    "def predict(self, x):\n",
    "    [o, s] = self.forward_propagation(x)\n",
    "    return np.argmax(x, axis = 0)\n",
    "\n",
    "RNN_numpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x,y):\n",
    "    L = 0\n",
    "    ##prbabilities using forward propagation\n",
    "    for i in range(len(x)):   \n",
    "        [o,s] = self.forward_propagation(x[i])\n",
    "        ###extracting probabilities of occurence using the index of actual\n",
    "        predicted_p = o[np.arange(len(y[i])), y[i]]\n",
    "        L += -1 * np.sum(np.log(predicted_p))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self,x,y):\n",
    "    loss = calculate_total_loss(self,x,y)\n",
    "    n = np.sum(len(y_i) for y_i in y)\n",
    "    return loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_numpy.calculate_total_loss = calculate_total_loss\n",
    "RNN_numpy.calculate_loss  = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_numpy(vocabulary_size, 100)\n",
    "####claculating the ideal loss if prediction were random\n",
    "print(\"theoritical loss if predictions where radom: {}\".format(np.log(vocabulary_size)))\n",
    "####calculateing the loss if the prediction where random\n",
    "print(\"actual loss if predictions where random: {}\".format(model.calculate_loss(X_train[:1000], Y_train[:1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads(self,x,y):\n",
    "    grade_W = np.zeros(self.W.shape)\n",
    "    grade_U = np.zeros(self.U.shape)\n",
    "    grade_V = np.zeros(self.V.shape)\n",
    "    T = len(y)\n",
    "    [o,s] = self.forward_propagation(x)\n",
    "    o_delta = o\n",
    "    o_delta[np.arange(len(y)), y] -= 1\n",
    "    \n",
    "    for t in np.arange(T)[::-1]:\n",
    "        grade_V += np.outer(o_delta[t], s[t].T)\n",
    "        delta_t = self.V.T.dot(o_delta[t]) * (1 - (s[t] ** 2))\n",
    "        for time_step in np.arange(max(0, t-self.truncate), t+1)[::-1]:\n",
    "\n",
    "            grade_W += np.outer(delta_t, s[time_step-1])\n",
    "            grade_U[:,x[t]] += delta_t\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[time_step-1] ** 2)\n",
    "    return [grade_U, grade_V, grade_W]\n",
    "    \n",
    "RNN_numpy.compute_grads = compute_grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_one_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.compute_grads(x, y)\n",
    "    # parameter update\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    "RNN_numpy.sgd_one_step = sgd_one_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, x, y, num_epoches,learning_rate, disp_loss_at):\n",
    "    loss = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(num_epoches):\n",
    "        L = self.calculate_loss(x,y)\n",
    "        loss.append((L, num_examples_seen))\n",
    "        if num_examples_seen%disp_loss_at == 0:\n",
    "            print(\"number of samples {}, loss = {}\".format(num_examples_seen, L))\n",
    "        if (len(loss) > 1):\n",
    "            if (loss[-1][0] > loss[-2][0]):\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"learning rate adjusted to {}\".format(learning_rate))\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(20)\n",
    "        \n",
    "        for m in range(len(y)):\n",
    "            self.sgd_one_step(x[m], y[m], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return loss\n",
    "RNN_numpy.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "model = RNN_numpy(vocabulary_size)\n",
    "arr = model.train(X_train[:10000], Y_train[:10000],10, 0.01, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pk\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(model):\n",
    "    new_samples = [1]\n",
    "    #while not new_samples[-1] == word_to_index[sentence_end_token]:\n",
    "    while not len(new_samples[-1]) == 10:\n",
    "        word_probs = model.forward_propagation(new_samples)\n",
    "        sample = np.choice(range(len(word_probs[0][-1]), word_probs[0][-1].ravel()))\n",
    "        if sample == word_to_index[unknown_token]:\n",
    "            sample = p.choice(range(len(word_probs[0][-1]), word_probs[0][-1].ravel()))\n",
    "            print(sample)\n",
    "        new_samples.append(sample)\n",
    "    sampled_indices = [index_to_word[x] for x in new_samples[1:-1]]\n",
    "    return sampled_indices\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = 10\n",
    "senten_min_length = 7\n",
    "\n",
    "for j in range(num_sentences):\n",
    "    sent = []\n",
    "    while len(sent) < senten_min_length:\n",
    "        sent = generate_sequence(model)\n",
    "    print(\" \".join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [34,1,16]\n",
    "for i in range(20):\n",
    "    [o, s] = model.forward_propagation(arr)\n",
    "    arr.append(np.argmax(o[-1], axis = -1))\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pk', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
