{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import csv\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Extracting sentences from csv file\n",
    "with open(\"reddit.csv\", \"r\", encoding=\"utf-8\") as file:\n",
    "    read = csv.reader(file, skipinitialspace = True)\n",
    "    next(read)\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in read])\n",
    "    sentences = [\"%s %s %s\"%(sentence_start_token,x,sentence_end_token) for x in sentences]\n",
    "    print(\"number of parsed sentences {}\".format(len(sentences)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Vocabulary info\n",
    "wordtokens = nltk.FreqDist(itertools.chain(*[nltk.word_tokenize(x) for x in sentences]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###most common words\n",
    "vocab = wordtokens.most_common(vocabulary_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###replacing words not in vocab as unknown_token\n",
    "tokenised_sent = [nltk.word_tokenize(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,sent in enumerate(tokenised_sent):\n",
    "    tokenised_sent[i] = [w if w in index_to_word else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenised_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenised_sent])\n",
    "Y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenised_sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations:\n",
    "         $s_t = \\tanh(Ux_t + Ws_{t-1})$  \n",
    "         $o_t = softmax(Vs_t)$\n",
    "dimensions:\n",
    "        - x_t: 8000 x 1\n",
    "        - U: 100 x 8000\n",
    "        - W: 100 x 100\n",
    "        - V: 8000 x 100\n",
    "        - o_t: 8000 x 1\n",
    "        - s_t: 100 x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_numpy:\n",
    "    def __init__(self, word_dim, hidden_dim):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        ##initialising parameters\n",
    "#         self.U = np.random.normal(size = (hidden_dim, word_dim))\n",
    "#         self.W = np.random.normal(size = (hidden_dim, hidden_dim))\n",
    "#         self.V = np.random.normal(size = (word_dim, hidden_dim))\n",
    "        \n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(self,array):\n",
    "            return np.exp(array) / np.sum(np.exp(array), axis=0)\n",
    "RNN_numpy.softmax = softmax\n",
    "        \n",
    "        ###method for forward propagation\n",
    "def forward_propagation(self, x):\n",
    "    ##number of time steps\n",
    "    T = len(x)\n",
    "    s = np.zeros((T+1, self.hidden_dim))\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    for t in range(T):\n",
    "        s[t] = np.tanh(self.U[:,x[t]] + np.dot(self.W, s[t-1])) ##(100 x 8000) . (8000 x 1) + (100 x 100) . (100 x 1) = (100 x 1)\n",
    "        o[t] = self.softmax(np.dot(self.V, s[t]))                    ##(8000 x 100) + (100 x 1) =(8000 x 1)\n",
    "\n",
    "    return [o,s]\n",
    "RNN_numpy.forward_propagation = forward_propagation\n",
    "\n",
    "#prediction\n",
    "def predict(self, x):\n",
    "    [o, s] = self.forward_propagation(x)\n",
    "    return np.argmax(x, axis = 0)\n",
    "\n",
    "RNN_numpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x,y):\n",
    "    L = 0\n",
    "    ##prbabilities using forward propagation\n",
    "    for i in range(len(x)):   \n",
    "        [o,s] = self.forward_propagation(x[i])\n",
    "        ###extracting probabilities of occurence using the index of actual\n",
    "        predicted_p = o[np.arange(len(y[i])), y[i]]\n",
    "        L += -1 * np.sum(np.log(predicted_p))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self,x,y):\n",
    "    loss = calculate_total_loss(self,x,y)\n",
    "    n = np.sum(len(y_i) for y_i in y)\n",
    "    return loss/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_numpy.calculate_total_loss = calculate_total_loss\n",
    "RNN_numpy.calculate_loss  = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN_numpy(vocabulary_size, 100)\n",
    "####claculating the ideal loss if prediction were random\n",
    "print(\"theoritical loss if predictions where radom: {}\".format(np.log(vocabulary_size)))\n",
    "####calculateing the loss if the prediction where random\n",
    "print(\"actual loss if predictions where random: {}\".format(model.calculate_loss(X_train[:1000], Y_train[:1000])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
